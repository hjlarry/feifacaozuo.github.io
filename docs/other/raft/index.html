<!DOCTYPE html>
<html lang="cn">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Raft 概述 对于分布式存储系统，通常会通过维护多个副本来进行容错，提高系统的可用性，那么就带来一个问题，如何维护多个副本的一致性？raft就是解决这个问题的算法。
在一个具有一致性、容错性的集群中，同一时刻所有节点对存储在其中的某个值应该有相同的结果，且当少数节点失效的时候，不影响集群的正常工作，当大多数集群中的节点失效的时候，集群则会停止服务（而非返回一个错误的结果）。
我们以MIT6.824的lab3中构建的容错键值服务为例，大概看一下基于raft算法的工作流程：
客户端发送Put/Get命令到集群中leader的K/V层，leader会把这个命令先添加到自己的日志中，同时把这个命令通过AppendEntriesRPC请求发送给自己的follower，并等待绝大多数follower的回复。如果大多数follower(这里也需要算上leader自己)都回复日志已提交，意味着即使失败了这条日志也不会丢失。那么leader就去可以执行这条命令，并把执行结果返回给客户端。在下一次leader发送请求(心跳或任意RPC请求)时，它会把自己已提交该日志(之前只是添加)附加进去，follower才知晓应该去执行这条命令。
为什么需要日志？ 日志服务是为了保持状态机(例如上图中的K/V层)的状态：
 日志可以让所有客户端发送的命令(可能有十个客户端同时发送命令)确定一个顺序，使得各个follower的执行都是统一的顺序，同时也使得leader能够确认各个follower有相同的日志 follower需要日志来临时存储一条临时的命令，它并不知道是否应该执行，直到它收到leader命令已提交 有时候会遇到follower接收不到消息、网络不畅等情况，日志可以帮助leader重新发送 当有服务器crash、掉线、重启或者有一个新的服务器加入时，日志可以帮助它重新执行一次完整的命令  leader选举 raft节点是在如图所示的三种状态中变换的，但在一个典型的稳态场景中，集群只有一个leader，其他都是follower。尽管我们希望系统一直是这样稳态的运行下去的，但是raft协议的目标就是容错。因此，会花费大量的时间来研究一些非典型的故障场景，如部分服务器崩溃或者断开连接等情况。
为什么需要一个leader？
在一些其他的集群一致性算法设计中，是可以不需要leader的，例如Paxos。但leader可以让raft更高效，每个follower都知道谁是leader，经过一轮消息的传递就可以处理一个事件。效率是最重要的原因，此外也有利于理解等其他原因。
Raft编排leader的顺序
每当有一个新的leader选举出，就会有一个新的term任期。在一个任期内，最多只会有一个leader，或者没有。这个term的值让集群内的服务器知晓谁是当前的leader，而不会去跟随一个被取代了的leader。
什么时候开始一轮新的选举？
每个节点都有一个Election Timer倒计时(每次收到消息，重置该时间)，倒计时内没有收到任何消息时，它就会发起一轮选举，当前的任期&#43;1、向其他节点发出投票请求。此时，可能是它自己掉线，产生了一个没有意义的选举，也可能是旧的leader掉线了。
如何确保每个term至多只有一个leader？
首先，必须获得集群中大多数节点的选票才能成为leader。其次，每个节点在当前的term中只能进行一次投票，如果它自己成为了候选人，它就投给自己；如果不是，它就投给第一个发送给它投票请求的节点。所以每个任期内，也至多只有一个节点能得到大多数其他节点的选票，即使在发生网络分裂、部分服务节点掉线的情况下。
其他节点如何知晓一个新的leader是谁？
对于新leader自己，它获得了过半的选票，自然知道自己已经选举成功，但是其他leader是不知道这点的。只有leader能发出AppendEntriesRPC请求，当follower收到一个更高term的该请求时，它们知道新的leader已经产生，不需要在进行下一轮的选举。
选举失败
选举失败只有两种情况，其一是大部分节点的掉线导致不能拿到大多数的选票；其二是多个候选人同时投票，使得没有人能获得大多数选票。发生失败以后，其实还是会导致Election Timer超时，从而term&#43;1，产生新的一轮选举。
如何避免投票的分散？
每个节点会选择一个随机的选举超时，谁的时间先到谁就会先去发起投票请求，接收到请求的节点就会变成follower。这种通过随机延时的方式在网络协议中是很常见的。
如何选择合适的选举超时时间？
这个时间不能太短，至少应该是几倍于心跳超时时间，这样如果网络中偶有丢包的话，可以避免不必要的选举。也需要足够长的时间让候选人在下一轮选举开始前计算出自己是否选举成功。也不能太长，因为在这个时间段，整个系统是没有leader的，也就是说是处于冻结状态无法响应客户端的。
旧的leader不知道新的leader被选举出怎么办？
也许是由于网络波动，旧的leader和少部分节点的网络在处理一部分客户端请求，新的leader和大部分节点的网络在处理一部分客户端请求，旧的leader并不知道新的leader产生，这难道不会造成旧的leader错误的执行？
旧的leader发出的AppendEntriesRPC请求只会让少部分节点收到，那么它就不会去提交任何新的日志项目，也就不会去执行命令并返回给客户端。
日志 只要leader存在，客户端只能和leader交互，而无法看到任何follower的状态或日志。那么当leader更替时，会不会出现一些异常？比如说存在两个不同的副本，丢失一些操作，重复一些操作等等。
在发生crash后日志是如何同步的？
假设有三个节点，在某次s3(leader)还没有发送完AppendEntries时crash了，那么它们的日志情况可能是这样的:
S1: 3 S2: 3 3 S3: 3 3 之后，s2在term4被选举为leader并接收到一个客户端请求添加了一条日志后crash，s3在term5发生了同样的事情，会形成这种情况:
 10 11 12 13 &lt;- log entry # S1: 3 S2: 3 3 4 S3: 3 3 5 此时，s3在term6被选举为leader，它如何同步follower的日志？"><meta property="og:title" content="raft" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="/docs/other/raft/" />

<title>raft | Home</title>
<link rel="icon" href="/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/book.min.63eb88daa545365405ecdbb21033286a325c60a36cfa6d22d21e7c3bc9286941.css" integrity="sha256-Y&#43;uI2qVFNlQF7NuyEDMoajJcYKNs&#43;m0i0h58O8koaUE=">
<script defer src="/cn.search.min.e9e7f19f81fc0df7ab1f4586881bd317c2c7e06717416f9bab8edaa07e45143c.js" integrity="sha256-6efxn4H8DferH0WGiBvTF8LH4GcXQW&#43;bq47aoH5FFDw="></script>
<link rel="alternate" type="application/rss+xml" href="/docs/other/raft/index.xml" title="Home" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body>
  <input type="checkbox" class="hidden" id="menu-control" />
  <main class="container flex">
    <aside class="book-menu">
      
  <nav>
<h2 class="book-brand">
  <a href="/"><img src="/logo.png" alt="Logo" /><span>Home</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="搜索" aria-label="搜索" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>











  <ul>
<li><strong>计算机基础</strong>
<ul>
<li><a href="/docs/sicp/hardware/">硬件</a></li>
<li><a href="/docs/sicp/software/">软件</a></li>
<li><a href="/docs/sicp/asm/">汇编</a></li>
</ul>
</li>
<li><strong>GO语言</strong>
<ul>
<li><a href="/docs/go/map/">字典</a></li>
<li><a href="/docs/go/closure/">闭包</a></li>
<li><a href="/docs/go/defer/">延迟调用</a></li>
<li><a href="/docs/go/goroutine/">并发调度</a></li>
<li><a href="/docs/go/alloc/">内存分配</a></li>
<li><a href="/docs/go/gc/">垃圾回收</a></li>
<li><a href="/docs/go/lock/">锁</a></li>
</ul>
</li>
<li><strong>Python语言</strong>
<ul>
<li><a href="/docs/python/descriptor/">描述符</a></li>
<li><a href="/docs/python/memory/">内存管理</a></li>
<li><a href="/docs/python/interpreter/">解释器</a></li>
<li><a href="/docs/python/tools/">技巧工具</a></li>
</ul>
</li>
<li><strong>Mysql</strong>
<ul>
<li><a href="/docs/mysql/query/">查询</a></li>
<li><a href="/docs/mysql/theory/">原理</a></li>
</ul>
</li>
<li><strong>其他</strong>
<ul>
<li><a href="/docs/other/git/">git</a></li>
<li><a href="/docs/other/docker/">docker</a></li>
<li><a href="/docs/other/raft/"class=active>raft</a></li>
<li><a href="/docs/other/protocol/">网络协议</a></li>
<li><a href="/docs/other/tools/">系统工具</a></li>
</ul>
</li>
</ul>










</nav>




  <script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script>


 
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>raft</strong>

  <label for="toc-control">
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
  </label>
</div>


  
 
      </header>

      
      
  <article class="markdown"><h1 id="raft">Raft</h1>
<h2 id="概述">概述</h2>
<p>对于分布式存储系统，通常会通过维护多个副本来进行容错，提高系统的可用性，那么就带来一个问题，如何维护多个副本的一致性？raft就是解决这个问题的算法。</p>
<p>在一个具有一致性、容错性的集群中，同一时刻所有节点对存储在其中的某个值应该有相同的结果，且当少数节点失效的时候，不影响集群的正常工作，当大多数集群中的节点失效的时候，集群则会停止服务（而非返回一个错误的结果）。</p>
<p>我们以<a href="https://pdos.csail.mit.edu/6.824/labs/lab-kvraft.html">MIT6.824的lab3</a>中构建的容错键值服务为例，大概看一下基于raft算法的工作流程：</p>
<p><img src="./images/raft_summary.png" alt="raft" /></p>
<p>客户端发送Put/Get命令到集群中leader的K/V层，leader会把这个命令先添加到自己的日志中，同时把这个命令通过<code>AppendEntries</code>RPC请求发送给自己的follower，并等待绝大多数follower的回复。如果大多数follower(这里也需要算上leader自己)都回复日志已提交，意味着即使失败了这条日志也不会丢失。那么leader就去可以执行这条命令，并把执行结果返回给客户端。在下一次leader发送请求(心跳或任意RPC请求)时，它会把自己已提交该日志(之前只是添加)附加进去，follower才知晓应该去执行这条命令。</p>
<h4 id="为什么需要日志">为什么需要日志？</h4>
<p>日志服务是为了保持状态机(例如上图中的K/V层)的状态：</p>
<ul>
<li>日志可以让所有客户端发送的命令(可能有十个客户端同时发送命令)确定一个顺序，使得各个follower的执行都是统一的顺序，同时也使得leader能够确认各个follower有相同的日志</li>
<li>follower需要日志来临时存储一条临时的命令，它并不知道是否应该执行，直到它收到leader命令已提交</li>
<li>有时候会遇到follower接收不到消息、网络不畅等情况，日志可以帮助leader重新发送</li>
<li>当有服务器crash、掉线、重启或者有一个新的服务器加入时，日志可以帮助它重新执行一次完整的命令</li>
</ul>
<h2 id="leader选举">leader选举</h2>
<p><img src="./images/raft_server_status.png" alt="raft" /><br>
raft节点是在如图所示的三种状态中变换的，但在一个典型的稳态场景中，集群只有一个leader，其他都是follower。尽管我们希望系统一直是这样稳态的运行下去的，但是raft协议的目标就是容错。因此，会花费大量的时间来研究一些非典型的故障场景，如部分服务器崩溃或者断开连接等情况。</p>
<p><strong>为什么需要一个leader？</strong><br>
在一些其他的集群一致性算法设计中，是可以不需要leader的，例如Paxos。但leader可以让raft更高效，每个follower都知道谁是leader，经过一轮消息的传递就可以处理一个事件。效率是最重要的原因，此外也有利于理解等其他原因。</p>
<p><strong>Raft编排leader的顺序</strong><br>
每当有一个新的leader选举出，就会有一个新的term任期。在一个任期内，最多只会有一个leader，或者没有。这个term的值让集群内的服务器知晓谁是当前的leader，而不会去跟随一个被取代了的leader。</p>
<p><strong>什么时候开始一轮新的选举？</strong><br>
每个节点都有一个<code>Election Timer</code>倒计时(每次收到消息，重置该时间)，倒计时内没有收到任何消息时，它就会发起一轮选举，当前的任期+1、向其他节点发出投票请求。此时，可能是它自己掉线，产生了一个没有意义的选举，也可能是旧的leader掉线了。</p>
<p><strong>如何确保每个term至多只有一个leader？</strong><br>
首先，必须获得集群中大多数节点的选票才能成为leader。其次，每个节点在当前的term中只能进行一次投票，如果它自己成为了候选人，它就投给自己；如果不是，它就投给第一个发送给它投票请求的节点。所以每个任期内，也至多只有一个节点能得到大多数其他节点的选票，即使在发生网络分裂、部分服务节点掉线的情况下。</p>
<p><strong>其他节点如何知晓一个新的leader是谁？</strong><br>
对于新leader自己，它获得了过半的选票，自然知道自己已经选举成功，但是其他leader是不知道这点的。只有leader能发出<code>AppendEntries</code>RPC请求，当follower收到一个更高term的该请求时，它们知道新的leader已经产生，不需要在进行下一轮的选举。</p>
<p><strong>选举失败</strong><br>
选举失败只有两种情况，其一是大部分节点的掉线导致不能拿到大多数的选票；其二是多个候选人同时投票，使得没有人能获得大多数选票。发生失败以后，其实还是会导致<code>Election Timer</code>超时，从而term+1，产生新的一轮选举。</p>
<p><strong>如何避免投票的分散？</strong><br>
每个节点会选择一个随机的选举超时，谁的时间先到谁就会先去发起投票请求，接收到请求的节点就会变成follower。这种通过随机延时的方式在网络协议中是很常见的。</p>
<p><strong>如何选择合适的选举超时时间？</strong><br>
这个时间不能太短，至少应该是几倍于心跳超时时间，这样如果网络中偶有丢包的话，可以避免不必要的选举。也需要足够长的时间让候选人在下一轮选举开始前计算出自己是否选举成功。也不能太长，因为在这个时间段，整个系统是没有leader的，也就是说是处于冻结状态无法响应客户端的。</p>
<p><strong>旧的leader不知道新的leader被选举出怎么办？</strong><br>
也许是由于网络波动，旧的leader和少部分节点的网络在处理一部分客户端请求，新的leader和大部分节点的网络在处理一部分客户端请求，旧的leader并不知道新的leader产生，这难道不会造成旧的leader错误的执行？<br>
旧的leader发出的<code>AppendEntries</code>RPC请求只会让少部分节点收到，那么它就不会去提交任何新的日志项目，也就不会去执行命令并返回给客户端。</p>
<h2 id="日志">日志</h2>
<p>只要leader存在，客户端只能和leader交互，而无法看到任何follower的状态或日志。那么当leader更替时，会不会出现一些异常？比如说存在两个不同的副本，丢失一些操作，重复一些操作等等。</p>
<p><strong>在发生crash后日志是如何同步的？</strong><br>
假设有三个节点，在某次s3(leader)还没有发送完<code>AppendEntries</code>时crash了，那么它们的日志情况可能是这样的:</p>
<pre><code>S1: 3
S2: 3 3
S3: 3 3
</code></pre><p>之后，s2在term4被选举为leader并接收到一个客户端请求添加了一条日志后crash，s3在term5发生了同样的事情，会形成这种情况:</p>
<pre><code>    10 11 12 13  &lt;- log entry #  
S1:  3  
S2:  3  3  4  
S3:  3  3  5  
</code></pre><p>此时，s3在term6被选举为leader，它如何同步follower的日志？</p>
<p>s3下次发出的<code>AppendEntries</code>请求会包含<code>prevLogIndex=12, prevLogTerm=5, Entries=[log6]</code>，s2接收到请求以后发现prevLogTerm不匹配，返回false，同样s1也返回false。s3收到返回后，其nextIndex[s2]和nextIndex[s1]的值均减1变为12，下次发出的请求包含<code>prevLogIndex=11, prevLogTerm=3, Entries=[log5, log6]</code>，s2会接受这个请求以及相应的日志，删掉自己index12上的log，并返回true。同理，s1最终也会在下一次请求中得到同步。最终所有节点的日志得到同步，nextIndex[s2]和nextIndex[s1]的值变为14。</p>
<p>也就是说，每个follower都会删掉自己尾部和leader不一致的日志项，并从那个不一致的地方开始接受leader的日志，使得其日志和leader同步。</p>
<p><strong>上例中s2的index为12、term为4的日志被丢掉不会有什么问题吗？</strong><br>
是没有问题的，这条日志并没有得到大多数服务器的确认，所以不会被提交，命令也不会被执行。那么有没有可能已经提交的日志项，在被新leader同步时被丢掉呢？实际上leader的选举机制会保证，要选举为leader的节点必须含有全部的已提交的日志。</p>
<p><strong>为什么不选择日志最长的服务器作为leader？</strong><br>
假设有这样一种情况:</p>
<pre><code>S1: 5 6 7
S2: 5 8
S3: 5 8
</code></pre><p>这种情况如何产生的呢？比如s1赢得了term6的选举，此时客户端发送一条命令，s1还没有来得及发送给其他节点时就已经crash了。s1很快的重启以后赶上了term7的选举并赢得了这轮选举，它依然收到一条客户端命令后还没发出去就crash，只是这次crash的时间很长。s2当选为了leader，并且应该为term8(因为s1要赢得term7的选举，s2或s3中有一个肯定给它投票了，投票时它的term就已经是7了)。此时s2接到客户端指令，并且也已发送给s3，s2和s3都将log8提交后发生了crash，此时s1也重连了进来。所以，简单的选择日志最长的服务器s1作为leader是不合适的，因为log8已经提交了，那么新的leader只能是s2或s3。</p>
<p>选举规则上，要求了候选人的最后日志项的term更高，或者term相同时，和follower比有更长或者相同的日志，follower才能投票给它。那么在这个例子中，s2和s3不会投票给s1，只会相互投票，其中之一赢得选举。s1的log6和log7将被丢弃，也不会对客户端发送相应的reply，客户端发现指令丢失后会重新发送丢掉的指令。</p>
<p><strong>如何快速的同步日志？</strong><br>
试想一下，在生产环境中，当新的leader产生需要和follower同步日志时，却需要像之前那样通过每次RPC请求向上回滚一个日志项来进行同步，该有多慢？</p>
<p>有这样一种方案能够解决(可能还有更好的方案)，它把同步分为以下三种情况:</p>
<pre><code># CASE 1:
S1:  4  5  5
S2:  4  6  6  6

# CASE 2:
S1:  4  4  4
S2:  4  6  6  6

# CASE 3:
S1:  4  
S2:  4  6  6  6
</code></pre><p>leader S2赢得了term6的选举，它仍然需要发送<code>AppendEntries</code>请求，包含<code>prevLogTerm=6</code>。此时follower S1如果拒绝了这次请求，还需要附上参数<code>XTerm</code>表示冲突日志项的term，以及<code>XIndex</code>表示这个term的第一个index，<code>XLen</code>为日志的长度。</p>
<p>对于case1，s1发送的参数包含<code>XTerm=5</code>，leader发现自己没有这个XTerm，则把nextIndex设为XIndex即可；对于case2，s1发送的参数包含<code>XTerm=4</code>，leader发现自己有这个XTerm，则把nextIndex设为自己日志中该term的最后一个日志项的index；对于case3，s1的日志长度过短，nextIndex为XLen。</p>
<h2 id="持久化">持久化</h2>
<p><strong>当一个服务器节点崩溃之后会发生什么？</strong><br>
raft能够继续工作，但崩溃的节点需要尽快的重新加入进来，否则更多的节点崩溃直至集群中大多数节点都挂掉就会导致集群无法工作。<br>
有两种重新加入的策略，要么替换为一个新的服务器，这需要传输整个log(或快照)给新的服务器，会很慢，通常发生在崩溃的节点无法恢复；要么重新启动崩溃的服务器，携带必要的信息，重新加入raft网络中继续工作。我们先来讨论第二种策略。</p>
<p><strong>如果一个服务器重启后重新加入，哪些信息是必须的？</strong><br>
根据raft论文中的描述，有<code>log[]</code>,<code>currentTerm</code>,<code>votedFor</code>这三个信息是必须的，这些信息应该存储在硬盘、SSD、带有电池的内存等设备中，且在每次修改时都需要保存一次。<br>
为什么需要<code>log[]</code>？log是raft中唯一用来记录app状态的，当服务器重启后用来恢复应用状态的就是存储在log中的一条条按顺序执行的命令。如果节点属于某次确认日志提交的大多数节点之一，那么重启后应当记住这点，用来保证任一未来的leader都能确认这条提交的日志。<br>
为什么需要<code>voteFor</code>？防止某个server投票给候选人A后崩溃重启，此时又接到候选人B发来的投票请求，导致相同term中既投给了A又投给了B，最终发生同一个term多个leader的情况。<br>
为什么需要<code>currentTerm</code>？确保term是一直增加的，因此每个term中至多有一个leader，它能检测到过时的leader和候选人的RPC消息。</p>
<p><strong>不去持久化<code>commitIndex</code>,<code>lastApplied</code>等信息正确吗？</strong><br>
这是可以的，因为崩溃重启并重新加入集群后，一旦收到当前leader发送的<code>AppendEntries</code>请求，leader就会告诉它正确的<code>commitIndex</code>。而<code>lastApplied</code>需要从0开始，因为论文中的raft算法是假定了应用本身不会保存任何持久化状态，需要通过重新执行每条日志中的指令来重新创建它的状态，这样效率是非常低的，使用快照机制并仅执行尾部的一些日志可以显著的提升效率。</p>
<p><strong>持久化的性能瓶颈</strong><br>
持久化通常是性能的瓶颈，因为一次数据写入硬盘的操作需要10ms，SSD也需要0.1ms，所以持久化就限制了我们每秒钟只能处理100个操作（SSD则是10000个，另一个瓶颈是局域网RPC，每次需要小于1ms的时间）。也有一些方式能够解决这个瓶颈，例如一些新的日志项是批量写入硬盘的、使用带有电源的内存设备等。</p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex justify-between">



  <div>
    
    <a class="flex align-center" href="https://github.com/hjlarry/hjlarry.github.io/commit/5d0a4436133862c05be45762aeada09717216ef5" title='最后修改者 hjlarry | May 13, 2020' target="_blank" rel="noopener">
      <img src="/svg/calendar.svg" class="book-icon" alt="Calendar" />
      <span>May 13, 2020</span>
    </a>
  </div>



  <div>
    <a class="flex align-center" href="https://github.com/hjlarry/hjlarry.github.io/edit/master/content/docs/other/raft/_index.md" target="_blank" rel="noopener">
      <img src="/svg/edit.svg" class="book-icon" alt="Edit" />
      <span>编辑本页</span>
    </a>
  </div>

</div>

 
        <br>
<div style="text-align: center;font-size:xx-small;">
    Powered by <a href="https://gohugo.io" target="_blank">Hugo</a> | Theme by <a
        href="https://github.com/alex-shpak/hugo-book" target="_blank">hugo-book</a>
</div>
      </footer>

      
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
  </main>

  
</body>

</html>












