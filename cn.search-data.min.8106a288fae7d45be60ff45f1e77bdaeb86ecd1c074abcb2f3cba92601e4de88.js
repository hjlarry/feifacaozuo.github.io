'use strict';(function(){const indexCfg={encode:false,tokenize:function(str){return str.replace(/[\x00-\x7F]/g,'').split('');}};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/sicp/hardware/','title':"硬件部分",'content':"计算机科学基础知识之硬件部分 现代计算机体系的基础理论部分仍然是70年代就已经产生的，这些年来一直没有多大突破，只是制造水平和加工工艺在逐步提升。未来除非是量子计算、生物计算等发展成熟，才可能使得计算机基础理论获得革命性突破。\n我们在应用软件开发领域，往往非常善于创造包装一些名词，但它们的本质就是那么回事，学习计算机体系的理论有助于我们抓住这些本质。\n体系结构 CPU要运行一个程序，它需要指令和数据，指令相当于你要干什么，数据是干这件事所需要的材料。\n冯诺依曼结构 冯诺依曼结构就是把指令也当做数据，把指令和数据放在一起。好处就是设计上会简单一些，可以使用一条总线集中存储；另外它还采用了二进制编码的线性地址空间。缺点就是它的数据传输效率远低于CPU的运算效率，因此需要缓存来提升效率。\n它的CPU有两套单元：\n ALU：算术逻辑单元，实现多组算术和逻辑运算。由与门和或门组成，进行二进制的算术运算，包括加、减、乘、与、或、非、异或。 CU：控制单元，用于程序流程的管理。从内存中取指令放入指令寄存器中分析并执行。  PS：在系统层面上，我们说的内存并不是指物理上的两根内存条，那个叫主存。比如硬盘上的缓存也是内存、打印机上也有缓存等等。\n哈佛结构 哈佛结构将指令和数据分开处理，指令和数据拥有不同的总线和地址空间，并行能力非常强，早期的大型计算机就采用这种结构。缺点就是早期电子电路昂贵，成本的问题使得它不适合于通用计算机没有推广开来。 现在纯粹的哈佛结构多用于数字信号处理器（DSP）、单片机等特定领域，它们的指令相对简单也没有缓存。\n对比总结 实际上现代计算机是对两种架构做了一些折中的。在内存中指令和数据是在一起的，在CPU的L1缓存中，会区分指令缓存和数据缓存，最终执行的时候指令和数据是从两个不同的地方出来的。另外冯诺依曼统一的地址空间也便于我们实现操作系统内存的管理、动态加载程序、JIT等。\n总线 我们通常所说的总线未必是电路板上一条条线路，它只是一种方式，可能是软件的，也可能是硬件的，代表了一整套的规范体系。就像高速公路交通网，包括了公路、收费站、加油站、维护人员、交通规则、信号灯指示牌，总线也是一样，除了数据通道，还包括了数据交换方式，比如从何处取（内存），从哪开始从哪结束有多长，取出来的是数据还是地址等。\n现在主流的总线标准为PCI/PCIe，该标准的带宽高，能探测硬件变更，支持热插拔。\n总线结构 按照不同的传输速度，我们会把总线分开，通过不同的速度来进行分流，使快速的部分拥有更多的流量带宽。北桥芯片处理快速设备，例如CPU、内存、显卡等，南桥芯片处理慢速设备，例如硬盘、网卡等。\n前端总线（FSB）负责CPU与北桥芯片的数据传输，代表CPU与外界数据的通讯速度，相当于CPU能处理的最高能力。而后端总线（BSB）负责与CPU内核通讯，其速度高于FSB。现在的芯片集成度越来越高的趋势也正是因为想减少各样的总线来提升各部件之间传输的速度。\n中央处理器 中央处理器是用来解释指令、处理数据的。编程实质上就是针对CPU而言的。例如我们往硬盘里写一个文件，就要知道文件名称、写在哪个地址、写入的长度等，即使这些工作不全是由CPU完成，CPU至少也要完成其中的调度工作。\n主频、外频 主频是内核工作频率，表达脉冲信号的震荡速度，与CPU的实际运算能力没有直接关系。而外频是内核与主板间的同步速度，同时也是内存和主板之间的同步速度。\n对现代的CPU而言，衡量其运算速度和性能要看各方面的性能指标，例如缓存、指令集等。很可能出现主频较高的CPU实际运算速度较低的情况。\n指令集 指令集是包含了基本数据类型、指令、寄存器、寻址模式、存储体系、中断和异常处理等打包的一套规范，是CPU真正能够理解的东西。\n我们通常分为精简指令集（RISC）和复杂指令集（CISC）。\n典型的复杂指令集就是x86，它的指令特别多，功能丰富，但每条指令的字长不等，也就造成了它需要先读出指令才知道后面的数据/参数有多长，执行速度相对较慢。而有一些虚拟机内会把指令设置为定长的，默认就是两个参数，这样它的缓存亲和性好，效率就高。x86这么设计有一些历史原因，当时还没有CPU缓存的概念。而精简指令集在早期反而是一种高大上的东西，它的指令和寻址方式少，格式统一，并行速度快，主要用于大型机和中高档服务器中。\nIntel认为自己定义的x86指令集由于历史原因等不好，于是在64位时代定义了IA-64指令集，但这种指令集和以前的x86并不兼容，它采用了模拟的方式去运行x86，这种方式在当时的windows2000等系统上运行的不好，所以微软持反对态度。此时AMD抓住了机会，它基于原有的386/IA-32标准做了扩充，也就有了现在的x86-64指令集，也可以叫做x64或AMD64。后来，Intel迫于其他厂商的压力，也去使用AMD64指令集，并在后来发展出了兼容的Intel 64，这种指令集和AMD64大部分是相同的。\n32位处理器的最大寻址为4GB（2的32次方），但64位处理器的最大寻址却不是2的64次方。我们知道存储单位的级别从小到大分别为Bytes、KB、MB、GB、TB、PB、EB，每级都等于前一级*1024，所以AMD64理论上可以访问16EB的地址空间，但目前的操作系统只支持到48位，也就是256TB的最大寻址，这种设定的根本原因就是能降低成本。\n此外，AMD64里增加了R8-R15的通用寄存器。\n寄存器和缓存 寄存器是所有存储体里最快的一个，因为数量少，所以可以直接给每个寄存器取个名字，而无需用地址。\n早期，数据由硬盘到内存，再经过前端总线直接到寄存器，因为前端总线传输的效率远低于CPU运算的效率，为了提高性能，我们在CPU内部，寄存器和内存之间增加了一层cache，使得前端总线每次传输更多的内容进入CPU。早期的CPU缓存只有L1，它将指令和数据分开；后来发展至L2，L2不区分指令和数据；再后来多核时代有了L3，能在一个物理处理器的多个核中共享。\n缓存只是解决性能问题的一个媒介，它本身有易丢失、易覆盖的特性，不能像寄存器和主存一样当做目标存储器使用。当CPU需要一个数据时，它会先去L1找，找不到则去L2，再找不到去L3找，再找不到就会去系统总线，找到以后批量的传到L3，再把命中的一部分填充至L2，再填充至L1，再返回给处理器。所以L1里有的数据L2、L3肯定有，每级缓存数据都是下一级的一部分。\n同时，基于时间局部性（正在被访问的数据可能近期再次被访问）、空间局部性（临近地址数据可能即将被访问）、顺序局部性（大部分指令是顺序执行），可以让缓存有较高的命中率。PS:我们宏观世界的缓存例如web服务中的memcache往往只是基于时间局部性，因为CPU是针对指令和字节的才去讨论其空间局部性和顺序局部性。\n我们写程序要尽可能的让其缓存亲和性更高，数据连续性更高，比如按一定长度对齐某些数据；对于性能要求高的程序，向操作系统申请锁死主存的一部分，避免被交换到硬盘当中去；甚至对于性能要求极致的场景，可以使用汇编以尽可能的使用寄存器。\n缓存由多个块组成，每个块我们称为cache line，每行的数据是连续的，每行的大小通常是64字节，行与行之间可能不连续，所以我们对齐数据的时候也是按行来对齐。同时，缓存中有很多的标志位，通过这些标志位去检查缓存是否更新，以决定是否需要置换回内存。\n多核 在一个处理器内集成多个独立实体物理内核。我们提及Core1、Core2的时候就表示多核，而CPU1、CPU2的时候则是多个CPU。多核能更好的在成本和性能上做出平衡，也不一定比多CPU慢，这主要看软件层面的优化。多核之间可以通过内部的L3缓存进行通讯或数据共享，而多CPU只能通过前端总线或额外建立其他外部通讯机制。\n多核架构又分为两种：\n对称多处理架构（SMP），多用于桌面端。每个处理器（在多核心处理器的例子中，对称多处理架构，将每一个核心都当成是独立的处理器）的地位是平等的，对资源的使用权限相同。好处是体系简单，缺点就是由于只有一个内存控制器，存在资源竞争的问题，一般是通过软硬件锁的机制解决，但随着处理器数量增加访问冲突就会增加，效率就会下降。\n非统一内存访问架构（NUMA），是一种为多处理器的电脑设计的内存架构。它将内存分散给多个处理器，处理器访问它自己的本地内存的速度比非本地内存（内存位于另一个处理器，或者是处理器之间共享的内存）快一些。它的扩展性更好，更适用于服务器，针对这种架构去编写程序效率也会更高。\n超线程 对于IO密集型任务，会出现CPU等待时间长的情况。那么超线程就是利用特殊的指令，在单个物理核内虚拟出多个逻辑处理器，在指令等待时做别的任务来减少闲置的时间，当然也需要额外的地方（AS，architectural state）保存当前上下文以切换任务。\n一般语言里提供的CPU数量都是逻辑处理器的数量，会将超线程虚拟的也算进去。例如python的multiprocessing.cpu_count()，或Go的runtime.NumCPU()。\n超线程多数时候可提升执行效率，但在有些情况下可能会导致性能下降，例如一些CPU密集的场合可能会对垃圾回收器造成负担；或者资源冲突时，依然需要等待，类似于同步锁。\n内存 内存严格来说叫内部存储器，不止包括物理上的内存条（即主存），硬盘、打印机等缓存也算作内部存储器。\n这里说的是主存，即随机存取存储器（RAM，Random Access Memory），它是与CPU直接交换数据的内部存储器。它可以随时读写，且速度很快，通常做为操作系统的临时数据存储介质。所谓的随机存取，是指当存储器的消息被读取或写入时，所需要的时间与这段信息所在的位置无关。\nDRAM 动态随机存取存储器。它具有结构简单，空间小，需要刷新的特点，往往被用于作为主存。\n需要刷新，是指电容器充满电后代表1，未充电的代表0。由于电容器或多或少有漏电的情形，若不作特别处理，电荷会渐渐随时间流失而使数据发生错误。刷新是指重新为电容器充电，弥补流失了的电荷。\nDDR是指具有双倍数据传输率的SDRAM（动态随机存取存储器），其数据传输速度为系统时脉的两倍。\nSRAM 静态随机存取存储器。它具有结构复杂，成本高，速度快的特点，一个典型的应用就是缓存。\n所谓的“静态”，是指这种存储器只要保持通电，里面储存的数据就可以恒常保持。相对之下，DRAM里面所储存的数据就需要周期性地更新。\n双通道 它在北桥内使用两个内存控制器分别控制一个通道，从而增加寻址和存储带宽，但是由于缓存的存在，并不需要这样的带宽，所以它对性能的提升往往感受不出来。在集成显卡的场景下会有一定帮助，因为集成显卡用内存作为显存，可以专门使用一条通道。\n显卡 转换显示信息，向显示器提供扫描信号。\n 2D芯片处理3D时需要CPU参与，称作软加速。 3D芯片自己完成，称作硬加速。  GPU是专门用来执行复杂的数学和几何计算的，它和CPU有什么区别呢？\nCPU需要很强的通用性来处理各种不同的数据类型，同时又要逻辑判断又会引入大量的分支跳转和中断的处理。这些都使得CPU的内部结构异常复杂。而GPU面对的则是类型高度统一的、相互无依赖的大规模数据和不需要被打断的纯净的计算环境。GPU的核数远超CPU，被称为众核（NVIDIA Fermi有512个核）。每个核拥有的缓存大小相对小，数字逻辑运算单元也少而简单。\n当程序员为CPU编写程序时，他们倾向于利用复杂的逻辑结构优化算法从而减少计算任务的运行时间，即Latency。当程序员为GPU编写程序时，则利用其处理海量数据的优势，通过提高总的数据吞吐量（Throughput）来掩盖Lantency。\n硬盘 主要的持久化存储媒介。\n分为固态硬盘（SSD）和机械硬盘（HDD）。SSD使⽤非易失性内存NAND Flash存储数据，无须电⼒维持。又分为单层SLC、双层MLC、三层TLC。单层最快，寿命最⻓，成本也最⾼。现在多使⽤MLC，TLC常作为U盘。\n其接口又分为SATA和SCSI。SATA是串行ATA，PC的标准接口。SCSI多用于小型机。\n其工作模式在BIOS可以设置，早期大多使用PIO，通过CPU执行IO指令读写数据，CPU要持续参与。现在多使用DMA，CPU只需要在开始和结束时参与，中间由DMA完成。\nIO cpu如何访问一大堆的设备？分为PMIO和MMIO。\nPMIO（Port Mapped I/O）是指端口映射输入输出，早期的时候会把主存和其他设备分开，分别用不同的CPU指令读写。因为使用了不同的地址总线，南桥一些慢速设备的访问不会拖累主存访问的效率。\n后来发现这种方式不好，64位时代内存地址空间充裕了就使用了MMIO（Memory Mapped I/O），它把内存地址空间分段，某一段设备用、程序用、操作系统用等，CPU使用相同的指令不同的地址就可操作设备，所有的设备都在监控地址总线，发现自己被访问就通过自身的缓存和MMU建立连接。\nBIOS 基本输入输出系统，启动后加载的第一个程序。\n最早没有操作系统的时候，写的程序就是面向BIOS的，BIOS在完成硬件初始化之后，会去执行硬盘上某个特定扇区的指令，所以只要把程序放在特定位置程序就可以启动。后来操作系统就放在那个位置，我们写的程序面向操作系统。\n它向操作系统提供系统参数、引导操作系统，但现代的操作系统比如mac就没有BIOS，而是直接控制硬件。\n"});index.add({'id':1,'href':'/docs/sicp/software/','title':"软件部分",'content':"hello world\n"});index.add({'id':2,'href':'/categories/','title':"Categories",'content':""});index.add({'id':3,'href':'/docs/','title':"Docs",'content':""});index.add({'id':4,'href':'/docs/go/goroutine/','title':"Go并发机制",'content':"Go并发调度 背景知识 GO和其他语言不同的就是我们很少在GO中听到多线程的概念，其提供的API中也没有创建线程这种东西。因为这门语言从用户写的第一行代码开始就是并发状态的。\n并发与并行 并发是指多个逻辑可以同时执行，把CPU时间分成不同的时间片段，这些时间片段分配给不同的逻辑，构成一个完整的CPU执行时间序列。 而并行是一种特殊的并发，不同逻辑由于CPU的多核，分配在不同的核上可以在物理时间上同时执行。这种状态其实很难实现，因为往往任一操作系统，它本身跑的程序非常多，远大于CPU核数。\n线程与协程 线程是执行单位，相当于工厂里的生产线。操作系统是按线程分配时间片，所以程序的线程越多获得的执行时间也就越长。线程是在系统空间实现的，而协程是在用户空间实现的，操作系统根本不知道协程。\n比如某个线程上有A、B两个任务，若A有死循环或者A等待网络响应等就会发生B被饿死的情况，为了避免这种情况，A就会主动让给B或者调度器去执行。这就是协程的工作方式，任务之间相互协商，属于协作式多任务系统，通常是在用户空间实现一个框架。而多线程是抢占式调度，不管某个程序会不会主动让出，当前时间片执行完就会被操作系统强迫分给其他线程。\n程序等于算法加数据，算法相当于一个解决问题的过程，数据又分为系统数据和用户数据。用户数据保存在用户堆栈上。操作系统为每个线程分配一个栈，大多用来保存局部变量，通常编译期就能确定，运行期通过寄存器访问，无需垃圾回收。而堆内存属于进程，进程内的线程共享，需要运行期动态分配以及垃圾回收。\n运行时 现代的编程语言创建一个线程往往是使用一个标准库或者第三方库提供API的，分配内存也往往会向操作系统提前申请一大块内存，通过这样一层抽象来减少用户态和内核态的切换来提升效率，我们把这层抽象叫做runtime（运行时）。它就像一个弱化版的操作系统，可以针对用户空间内的代码，结合当前语言的特性做大量的优化。\nGo运行时第一个抽象出的概念就是P（Processor），相当于处理器。物理上有多少个CPU、有多少个核，runtime并不关心，它是在OS上的一层抽象，os才是在硬件的上一层抽象。runtime认为在当前的环境内只有一个程序，所以我们可以通过P来设定并发的数量，同时能执行这个程序内的多少个并发任务。\n第二个抽象是M（Machine），对应了一个系统线程，是对线程的包装，也就是说P控制了同时有多少个M在执行。它是实际执行体，和P绑定，以调度循环方式不断执行G并发任务。\n第三个抽象就是G（Goroutine），实际上就是任务载体，或者说资源包，包括了函数地址，需要的参数，所需的内存。当我们使用go func(){}()时，实际上就是创建了一个G对象。\n为什么G需要内存，按说M相当于线程也就应该有了栈内存？实际上它们都有自己的内存，G中的内存为G.stack（默认大小2KB），M中的内存为G0。G在M上运行，就像是列车在线路上运行，线路本身也需要去投入资源维护。而把两块内存分开，是因为M所需的内存比较连续、相对固定、逻辑完整，G却会因为各种各样的原因或者异常可能会调度到别的M上去。\nG、M、P共同构成了多任务并发执行的基本模式，P用来控制同时有多少个并发任务执行，M对应到某个线程，G代表了go func语句翻译的一个任务包，最终还得有个调度器统合起来，把G放到合适的M上去执行。\n任务平衡 当我们在一个for循环中创建了成千上万个并发任务时，它们并不是立即执行的，而是打包成一个个G对象保存在两个队列中（P本地队列和G全局队列）。\n假设当前只有4个P，在main函数执行的时候就需要一个P1/M1绑定体，main中创建的其他go func就会打包成G对象放在P1.queue中。也就是说任一M内创建的G都会保存在当前这个P的本地队列中，为什么不能放在P2、P3、P4的队列中？放在别的队列就需要去判断这个P是不是闲置的，还可能需要加锁等等，会变得很复杂。\n那么如果在main中创建了1000个G，它们就得等P1/M1中当前的任务执行完了才会得到执行，可能P2、P3、P4都是闲置的，这明显不合理。如何在多个P之间去平衡任务呢？使用了两种方法，一种是规定了每个P本地队列只能放256个G，一次放的过多时会按一定规则比如放一半到全局队列中去；另一种是某个P若闲置了就会在全局队列中去找（可能有很多P都在全局中找，就需要排队去找），找到了就把一部分任务移动到自己的本地队列中，没有找到就会去其他P中偷一部分任务过来，从全局队里或其他P中偷都是需要加锁的，效率相对会低一些。\n这样的平衡方式也就决定了我们没有办法确定哪个方法先执行，哪个后执行，除非我们自己写逻辑去判断先后。我们再来看一个关于执行顺序的示例:\nfunc main() {\rruntime.GOMAXPROCS(1) // 设置P为1\r for i := 0; i \u0026lt; 10; i++ {\rgo func(id int) { // 创建10个G\r time.Sleep(time.Second)\rfmt.Println(id)\r}(i)\r}\rtime.Sleep(time.Second * 2)\r}\r执行结果:\n[ubuntu] ~/.mac/gocode $ go run goroutine.go\r9\r0\r1\r2\r3\r4\r5\r6\r7\r8\r为什么当P为1的时候，它不是顺序输出的，9总是在第一个？\n每个P的本地队列中其实包含两个部分，runnext[1]和runq[256]。当我们每次添加一个任务的时候，它会先放在runnext中，再添加一个任务时，会把新添加的放在runnext，之前添加的放在runq中。runnext总是保留用户最后创建的任务，执行的时候先查runnext去执行。\n为什么要有runnext的设计？\n假设只创建了一个并发任务，也放在runq中让别的P去抢没有必要，而且大多数情况下我们不会去批量创建G；另外若runq既用来P1执行又让P2、P3去偷，那就又会涉及到加锁。\n那么为什么是G9放在runnext，而不是G0？\n因为放在runnext以后我们无法保证还有多少逻辑执行完才轮到它，就可能会runq中的任务都被偷走了且执行完了G0才会执行，这对G0很不公平。\n显然任务被分成了三个性能层次，runnext是完全私有的，runq属于原子操作（原子操作对CPU来讲也是锁，锁的是地址总线），Global属于一定要加mutex锁的，这三个层次产生资源竞争的可能性逐步增大。\n调度执行 P与M如何解绑 当我们创建一个G的时候，实际上是背后的调度器在当前M上的G0去执行的，它发现有新的任务出现时，会发出一个唤醒信号，去检查有没有P空闲的，以及有没有M是休眠状态的？若P空闲且没有休眠的M，就会去创建一个M对象。所以唤醒操作要有意义，就得有P闲着没事干。\n那么M是怎样变为休眠状态的？\n当一个P和M绑定之后，它会进入到一个调度程序（Schedule函数），调度程序会去找G对象（按runnext、runq、Global、other P的顺序），找到之后内存由G0上执行切换到G.stack上去执行，执行完成之后进入收尾阶段，清理现场把G当做一个包装对象让它能重复使用。然后重新回到Schedule函数形成一个调度循环。\n这个循环可能因为找不到G对象而中断，比如说当前就只有一个任务。那么P和M就会解绑，M会进入休眠状态。\n还有一种P和M解绑的情况，比如当前在进行一个系统调用，而这个系统调用花了很长时间，调度器就会把这个P拿走干别的事，而M压根不知道，因为它在内核态，等系统调用结束以后M发现找不到P，那它就没法继续执行，只能把当前的任务状态保存回G.stack，在把执行一半的任务重新放回队列，M再次进入休眠状态，执行一半的任务再下次遇到P/M时接着执行。\n这就可能导致一个问题，创建出大量的空闲的M，不会被回收。M是会在操作系统内核中创建一个线程，尽管这种休眠状态下的M不会被CPU分配时间片，但仍然会占用管理资源，另外每个M上都带着G0内存，相当于资源泄漏了。我们通过如下代码来模拟这种情况：\nfunc main(){\rfor i :=0;i\u0026lt;1000;i++{\rgo func(){\rruntime.LockOSThread() //通过锁模拟系统调用\r defer runtime.UnlockOSThread()\rtime.Sleep(time.Second*5)\r}()\r}\rtime.Sleep(time.Minute)\r}\r通过go build test.go \u0026amp;\u0026amp; GODEBUG=schedtrace=1000 ./test运行：\nSCHED 0ms: gomaxprocs=4 idleprocs=2 threads=5 spinningthreads=1 idlethreads=2 runqueue=0 [0 0 0 0]\rSCHED 1002ms: gomaxprocs=4 idleprocs=4 threads=1010 spinningthreads=0 idlethreads=4 runqueue=0 [0 0 0 0]\rSCHED 2008ms: gomaxprocs=4 idleprocs=4 threads=1010 spinningthreads=0 idlethreads=4 runqueue=0 [0 0 0 0]\rSCHED 3013ms: gomaxprocs=4 idleprocs=4 threads=1010 spinningthreads=0 idlethreads=4 runqueue=0 [0 0 0 0]\rSCHED 4014ms: gomaxprocs=4 idleprocs=4 threads=1010 spinningthreads=0 idlethreads=4 runqueue=0 [0 0 0 0]\rSCHED 5015ms: gomaxprocs=4 idleprocs=4 threads=1010 spinningthreads=0 idlethreads=127 runqueue=0 [0 0 0 0]\rSCHED 6017ms: gomaxprocs=4 idleprocs=4 threads=1010 spinningthreads=0 idlethreads=1007 runqueue=0 [0 0 0 0]\rSCHED 7025ms: gomaxprocs=4 idleprocs=4 threads=1010 spinningthreads=0 idlethreads=1007 runqueue=0 [0 0 0 0]\rSCHED 8027ms: gomaxprocs=4 idleprocs=4 threads=1010 spinningthreads=0 idlethreads=1007 runqueue=0 [0 0 0 0]\rSCHED 9029ms: gomaxprocs=4 idleprocs=4 threads=1010 spinningthreads=0 idlethreads=1007 runqueue=0 [0 0 0 0]\rSCHED 10031ms: gomaxprocs=4 idleprocs=4 threads=1010 spinningthreads=0 idlethreads=1007 runqueue=0 [0 0 0 0]\r我们发现任务开始的时候共创建了1010个线程，任务执行完以后，仍然有1007个休眠的线程。当我们把runtime.LockOSThread()注释掉，重新运行：\nSCHED 0ms: gomaxprocs=4 idleprocs=1 threads=5 spinningthreads=1 idlethreads=1 runqueue=0 [48 49 133 0]\rSCHED 1004ms: gomaxprocs=4 idleprocs=4 threads=9 spinningthreads=0 idlethreads=4 runqueue=0 [0 0 0 0]\rSCHED 2008ms: gomaxprocs=4 idleprocs=4 threads=9 spinningthreads=0 idlethreads=4 runqueue=0 [0 0 0 0]\rSCHED 3010ms: gomaxprocs=4 idleprocs=4 threads=9 spinningthreads=0 idlethreads=4 runqueue=0 [0 0 0 0]\rSCHED 4015ms: gomaxprocs=4 idleprocs=4 threads=9 spinningthreads=0 idlethreads=4 runqueue=0 [0 0 0 0]\rSCHED 5026ms: gomaxprocs=4 idleprocs=4 threads=9 spinningthreads=0 idlethreads=7 runqueue=0 [0 0 0 0]\rSCHED 6031ms: gomaxprocs=4 idleprocs=4 threads=9 spinningthreads=0 idlethreads=7 runqueue=0 [0 0 0 0]\r发现没有系统调用，那就根本不会有那么多线程。我们再把runtime.LockOSThread()保留，defer runtime.UnlockOSThread()去掉，运行结果如下：\nSCHED 0ms: gomaxprocs=4 idleprocs=2 threads=5 spinningthreads=1 idlethreads=2 runqueue=0 [0 0 152 0]\rSCHED 1000ms: gomaxprocs=4 idleprocs=4 threads=1010 spinningthreads=0 idlethreads=4 runqueue=0 [0 0 0 0]\rSCHED 2001ms: gomaxprocs=4 idleprocs=4 threads=1010 spinningthreads=0 idlethreads=4 runqueue=0 [0 0 0 0]\rSCHED 3008ms: gomaxprocs=4 idleprocs=4 threads=1010 spinningthreads=0 idlethreads=4 runqueue=0 [0 0 0 0]\rSCHED 4016ms: gomaxprocs=4 idleprocs=4 threads=1010 spinningthreads=0 idlethreads=4 runqueue=0 [0 0 0 0]\rSCHED 5019ms: gomaxprocs=4 idleprocs=0 threads=938 spinningthreads=0 idlethreads=4 runqueue=0 [18 1 3 7]\rSCHED 6027ms: gomaxprocs=4 idleprocs=4 threads=13 spinningthreads=0 idlethreads=10 runqueue=0 [0 0 0 0]\rSCHED 7037ms: gomaxprocs=4 idleprocs=4 threads=13 spinningthreads=0 idlethreads=10 runqueue=0 [0 0 0 0]\rSCHED 8042ms: gomaxprocs=4 idleprocs=4 threads=13 spinningthreads=0 idlethreads=10 runqueue=0 [0 0 0 0]\rSCHED 9044ms: gomaxprocs=4 idleprocs=4 threads=13 spinningthreads=0 idlethreads=10 runqueue=0 [0 0 0 0]\rcSCHED 10047ms: gomaxprocs=4 idleprocs=4 threads=13 spinningthreads=0 idlethreads=10 runqueue=0 [0 0 0 0]\r我们发现那些创建的线程是会被回收的，线程没有被解锁意味着线程的状态没有被解除而陷入了死锁状态，线程不能再去接收新的任务没有存在的意义自然会被杀掉。\n综上，我们发现P往往是恒定的，而G和M是可复用的，复用虽然可能造成资源的浪费，但它避免了重新创建时的可能造成的竞争效应。对于一些长期运行的东西，我们需要再创建还是释放之间做一些权衡。\n任务饿死 假设当前的P/M正在执行一个G，这时候G里面创建了一个G1，G1会被放到当前P的runnext中，但它可能迟迟得不到执行被饿死，因为G中可能还有大量的逻辑代码执行完才轮到runnext，这显然是不合理的。怎么解决这个问题呢？是否可以让当前这个P/M交替的执行G和G1？P不是真正的CPU，没法实现基于时间片的抢占式调度，只能实现类似于协程那样的协作式调度。很多语言里都使用类似于Gosched()这样的函数来主动交出执行权，但Go中却很少见；还有一种形式是runtime带一个计数器，每执行一个任务后累加计数，当到达一个指定的计数就会被认为是使用完了时间片，向当前执行的P/M发出一个抢占式的信号，然后G主动让出执行权限。Go到底是怎样做的？我们来看如下示例：\nfunc main(){\rruntime.GOMAXPROCS(1)\rfor i:=0;i\u0026lt;3;i++{\rgo func(id int){\rprintln(id)\rx := 0\rfor{ // 死循环\r x++\r//print()\r }\r}(i)\r}\rtime.Sleep(time.Second)\r}\r使用GODEBUG=schedtrace=1000,scheddetail=1 ./test可以查看运行期间GMP的状态。我们模拟了只有一个P/M，这时候创建了3个G，让第一个G执行的过程中进入死循环，运行结果就是只打印出了任务0，其他G被饿死了。但当我们在死循环内x++后面加入一个函数，则任务0、1、2都会被打印出。问题就出在这个函数上。\n我们先使用一个简单的函数来观察：\n//go:noinline\rfunc test(){\rprintln()\r}\rfunc main(){\rtest()\r}\r使用go build \u0026amp;\u0026amp; go tool objdump -s \u0026quot;main\\.test\u0026quot; test反汇编：\nTEXT main.test(SB) /mnt/hgfs/disk/test.go\rtest.go:5\t0x4525b0\t64488b0c25f8ffffff\tMOVQ FS:0xfffffff8, CX\ttest.go:5\t0x4525b9\t483b6110\tCMPQ 0x10(CX), SP\ttest.go:5\t0x4525bd\t7624\tJBE 0x4525e3\ttest.go:5\t0x4525bf\t4883ec08\tSUBQ $0x8, SP\ttest.go:5\t0x4525c3\t48892c24\tMOVQ BP, 0(SP)\ttest.go:5\t0x4525c7\t488d2c24\tLEAQ 0(SP), BP\ttest.go:6\t0x4525cb\te80031fdff\tCALL runtime.printlock(SB)\ttest.go:6\t0x4525d0\te88b33fdff\tCALL runtime.printnl(SB)\ttest.go:6\t0x4525d5\te87631fdff\tCALL runtime.printunlock(SB)\ttest.go:7\t0x4525da\t488b2c24\tMOVQ 0(SP), BP\ttest.go:7\t0x4525de\t4883c408\tADDQ $0x8, SP\ttest.go:7\t0x4525e2\tc3\tRET\ttest.go:5\t0x4525e3\te8187bffff\tCALL runtime.morestack_noctxt(SB)\ttest.go:5\t0x4525e8\tebc6\tJMP main.test(SB)\t我们发现头部的三条指令和尾部的两条指令都是编译器插入的。runtime.morestack_noctxt会做两件事情，一是检查当前栈帧空间是否足够，如果不够可以帮助扩容；二是检查是否有人发出了抢占式调度信号，如果发现了信号，它就让出执行权限。函数前使用go:nosplit可以禁止编译器插入这样的指令。\n"});index.add({'id':5,'href':'/','title':"Introduction",'content':"非法操作的知识库 主要用于记录学习过程中的重要知识点。\n关于作者 \rGithub\r\rTwitter\r\rEmail\r\rTelegram\r\r"});index.add({'id':6,'href':'/tags/','title':"Tags",'content':""});})();