---
title: "raft"
draft: false
bookToc: false
---

# Raft


概述
-------

对于分布式存储系统，通常会通过维护多个副本来进行容错，提高系统的可用性，那么就带来一个问题，如何维护多个副本的一致性？raft就是解决这个问题的算法。

在一个具有一致性、容错性的集群中，同一时刻所有节点对存储在其中的某个值应该有相同的结果，且当少数节点失效的时候，不影响集群的正常工作，当大多数集群中的节点失效的时候，集群则会停止服务（而非返回一个错误的结果）。

我们以[MIT6.824的lab3](https://pdos.csail.mit.edu/6.824/labs/lab-kvraft.html)中构建的容错键值服务为例，大概看一下基于raft算法的工作流程：

![raft](./images/raft_summary.png)

客户端发送Put/Get命令到集群中leader的K/V层，leader会把这个命令先添加到自己的日志中，同时把这个命令通过`AppendEntries`RPC请求发送给自己的follower，并等待绝大多数follower的回复。如果大多数follower(这里也需要算上leader自己)都回复日志已提交，意味着即使失败了这条日志也不会丢失。那么leader就去可以执行这条命令，并把执行结果返回给客户端。在下一次leader发送请求(心跳或任意RPC请求)时，它会把自己已提交该日志(之前只是添加)附加进去，follower才知晓应该去执行这条命令。

#### 为什么需要日志？
日志服务是为了保持状态机(例如上图中的K/V层)的状态：
* 日志可以让所有客户端发送的命令(可能有十个客户端同时发送命令)确定一个顺序，使得各个follower的执行都是统一的顺序，同时也使得leader能够确认各个follower有相同的日志
* follower需要日志来临时存储一条临时的命令，它并不知道是否应该执行，直到它收到leader命令已提交
* 有时候会遇到follower接收不到消息、网络不畅等情况，日志可以帮助leader重新发送
* 当有服务器crash、掉线、重启或者有一个新的服务器加入时，日志可以帮助它重新执行一次完整的命令

leader选举
-------

![raft](./images/raft_server_status.png)  
raft节点是在如图所示的三种状态中变换的，但在一个典型的稳态场景中，集群只有一个leader，其他都是follower。尽管我们希望系统一直是这样稳态的运行下去的，但是raft协议的目标就是容错。因此，会花费大量的时间来研究一些非典型的故障场景，如部分服务器崩溃或者断开连接等情况。

**为什么需要一个leader？**  
在一些其他的集群一致性算法设计中，是可以不需要leader的，例如Paxos。但leader可以让raft更高效，每个follower都知道谁是leader，经过一轮消息的传递就可以处理一个事件。效率是最重要的原因，此外也有利于理解等其他原因。

**Raft编排leader的顺序**  
每当有一个新的leader选举出，就会有一个新的term任期。在一个任期内，最多只会有一个leader，或者没有。这个term的值让集群内的服务器知晓谁是当前的leader，而不会去跟随一个被取代了的leader。

**什么时候开始一轮新的选举？**  
每个节点都有一个`Election Timer`倒计时(每次收到消息，重置该时间)，倒计时内没有收到任何消息时，它就会发起一轮选举，当前的任期+1、向其他节点发出投票请求。此时，可能是它自己掉线，产生了一个没有意义的选举，也可能是旧的leader掉线了。

**如何确保每个term至多只有一个leader？**  
首先，必须获得集群中大多数节点的选票才能成为leader。其次，每个节点在当前的term中只能进行一次投票，如果它自己成为了候选人，它就投给自己；如果不是，它就投给第一个发送给它投票请求的节点。所以每个任期内，也至多只有一个节点能得到大多数其他节点的选票，即使在发生网络分裂、部分服务节点掉线的情况下。

**其他节点如何知晓一个新的leader是谁？**  
对于新leader自己，它获得了过半的选票，自然知道自己已经选举成功，但是其他leader是不知道这点的。只有leader能发出`AppendEntries`RPC请求，当follower收到一个更高term的该请求时，它们知道新的leader已经产生，不需要在进行下一轮的选举。

**选举失败**  
选举失败只有两种情况，其一是大部分节点的掉线导致不能拿到大多数的选票；其二是多个候选人同时投票，使得没有人能获得大多数选票。发生失败以后，其实还是会导致`Election Timer`超时，从而term+1，产生新的一轮选举。

**如何避免投票的分散？**  
每个节点会选择一个随机的选举超时，谁的时间先到谁就会先去发起投票请求，接收到请求的节点就会变成follower。这种通过随机延时的方式在网络协议中是很常见的。

**如何选择合适的选举超时时间？**  
这个时间不能太短，至少应该是几倍于心跳超时时间，这样如果网络中偶有丢包的话，可以避免不必要的选举。也需要足够长的时间让候选人在下一轮选举开始前计算出自己是否选举成功。也不能太长，因为在这个时间段，整个系统是没有leader的，也就是说是处于冻结状态无法响应客户端的。

**旧的leader不知道新的leader被选举出怎么办？**  
也许是由于网络波动，旧的leader和少部分节点的网络在处理一部分客户端请求，新的leader和大部分节点的网络在处理一部分客户端请求，旧的leader并不知道新的leader产生，这难道不会造成旧的leader错误的执行？  
旧的leader发出的`AppendEntries`RPC请求只会让少部分节点收到，那么它就不会去提交任何新的日志项目，也就不会去执行命令并返回给客户端。

日志
-------
只要leader存在，客户端只能和leader交互，而无法看到任何follower的状态或日志。那么当leader更替时，会不会出现一些异常？比如说存在两个不同的副本，丢失一些操作，重复一些操作等等。

**在发生crash后日志是如何同步的？**  
假设有三个节点，在某次s3(leader)还没有发送完`AppendEntries`时crash了，那么它们的日志情况可能是这样的:
```
S1: 3
S2: 3 3
S3: 3 3
```
之后，s2在term4被选举为leader并接收到一个客户端请求添加了一条日志后crash，s3在term5发生了同样的事情，会形成这种情况:
```
    10 11 12 13  <- log entry #  
S1:  3  
S2:  3  3  4  
S3:  3  3  5  
```
此时，s3在term6被选举为leader，它如何同步follower的日志？

s3下次发出的`AppendEntries`请求会包含`prevLogIndex=12, prevLogTerm=5, Entries=[log6]`，s2接收到请求以后发现prevLogTerm不匹配，返回false，同样s1也返回false。s3收到返回后，其nextIndex[s2]和nextIndex[s1]的值均减1变为12，下次发出的请求包含`prevLogIndex=11, prevLogTerm=3, Entries=[log5, log6]`，s2会接受这个请求以及相应的日志，删掉自己index12上的log，并返回true。同理，s1最终也会在下一次请求中得到同步。最终所有节点的日志得到同步，nextIndex[s2]和nextIndex[s1]的值变为14。

也就是说，每个follower都会删掉自己尾部和leader不一致的日志项，并从那个不一致的地方开始接受leader的日志，使得其日志和leader同步。

**上例中s2的index为12、term为4的日志被丢掉不会有什么问题吗？**  
是没有问题的，这条日志并没有得到大多数服务器的确认，所以不会被提交，命令也不会被执行。那么有没有可能已经提交的日志项，在被新leader同步时被丢掉呢？实际上leader的选举机制会保证，要选举为leader的节点必须含有全部的已提交的日志。

**为什么不选择日志最长的服务器作为leader？**  
假设有这样一种情况:
```
S1: 5 6 7
S2: 5 8
S3: 5 8
```
这种情况如何产生的呢？比如s1赢得了term6的选举，此时客户端发送一条命令，s1还没有来得及发送给其他节点时就已经crash了。s1很快的重启以后赶上了term7的选举并赢得了这轮选举，它依然收到一条客户端命令后还没发出去就crash，只是这次crash的时间很长。s2当选为了leader，并且应该为term8(因为s1要赢得term7的选举，s2或s3中有一个肯定给它投票了，投票时它的term就已经是7了)。此时s2接到客户端指令，并且也已发送给s3，s2和s3都将log8提交后发生了crash，此时s1也重连了进来。所以，简单的选择日志最长的服务器s1作为leader是不合适的，因为log8已经提交了，那么新的leader只能是s2或s3。

选举规则上，要求了候选人的最后日志项的term更高，或者term相同时，和follower比有更长或者相同的日志，follower才能投票给它。那么在这个例子中，s2和s3不会投票给s1，只会相互投票，其中之一赢得选举。s1的log6和log7将被丢弃，也不会对客户端发送相应的reply，客户端发现指令丢失后会重新发送丢掉的指令。

**如何快速的同步日志？**  
试想一下，在生产环境中，当新的leader产生需要和follower同步日志时，却需要像之前那样通过每次RPC请求向上回滚一个日志项来进行同步，该有多慢？

有这样一种方案能够解决(可能还有更好的方案)，它把同步分为以下三种情况:
```
# CASE 1:
S1:  4  5  5
S2:  4  6  6  6

# CASE 2:
S1:  4  4  4
S2:  4  6  6  6

# CASE 3:
S1:  4  
S2:  4  6  6  6
```
leader S2赢得了term6的选举，它仍然需要发送`AppendEntries`请求，包含`prevLogTerm=6`。此时follower S1如果拒绝了这次请求，还需要附上参数`XTerm`表示冲突日志项的term，以及`XIndex`表示这个term的第一个index，`XLen`为日志的长度。

对于case1，s1发送的参数包含`XTerm=5`，leader发现自己没有这个XTerm，则把nextIndex设为XIndex即可；对于case2，s1发送的参数包含`XTerm=4`，leader发现自己有这个XTerm，则把nextIndex设为自己日志中该term的最后一个日志项的index；对于case3，s1的日志长度过短，nextIndex为XLen。

持久化
-------