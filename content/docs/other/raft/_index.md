---
title: "raft"
draft: false
bookToc: false
---

# Raft


概述
-------

对于分布式存储系统，通常会通过维护多个副本来进行容错，提高系统的可用性，那么就带来一个问题，如何维护多个副本的一致性？raft就是解决这个问题的算法。

在一个具有一致性、容错性的集群中，同一时刻所有节点对存储在其中的某个值应该有相同的结果，且当少数节点失效的时候，不影响集群的正常工作，当大多数集群中的节点失效的时候，集群则会停止服务（而非返回一个错误的结果）。

我们以[MIT6.824的lab3](https://pdos.csail.mit.edu/6.824/labs/lab-kvraft.html)中构建的容错键值服务为例，大概看一下基于raft算法的工作流程：

![raft](./images/raft_summary.png)

客户端发送Put/Get命令到集群中leader的K/V层，leader会把这个命令先添加到自己的日志中，同时把这个命令通过`AppendEntries`RPC请求发送给自己的follower，并等待绝大多数follower的回复。如果大多数follower(这里也需要算上leader自己)都回复日志已提交，意味着即使失败了这条日志也不会丢失。那么leader就去可以执行这条命令，并把执行结果返回给客户端。在下一次leader发送请求(心跳或任意RPC请求)时，它会把自己已提交该日志(之前只是添加)附加进去，follower才知晓应该去执行这条命令。

#### 为什么需要日志？
日志服务是为了保持状态机(例如上图中的K/V层)的状态：
* 日志可以让所有客户端发送的命令(可能有十个客户端同时发送命令)确定一个顺序，使得各个follower的执行都是统一的顺序，同时也使得leader能够确认各个follower有相同的日志
* follower需要日志来临时存储一条临时的命令，它并不知道是否应该执行，直到它收到leader命令已提交
* 有时候会遇到follower接收不到消息、网络不畅等情况，日志可以帮助leader重新发送
* 当有服务器crash、掉线、重启或者有一个新的服务器加入时，日志可以帮助它重新执行一次完整的命令

leader选举
-------

![raft](./images/raft_server_status.png)  
raft节点是在如图所示的三种状态中变换的，但在一个典型的稳态场景中，集群只有一个leader，其他都是follower。尽管我们希望系统一直是这样稳态的运行下去的，但是raft协议的目标就是容错。因此，会花费大量的时间来研究一些非典型的故障场景，如部分服务器崩溃或者断开连接等情况。

**为什么需要一个leader？**  
在一些其他的集群一致性算法设计中，是可以不需要leader的，例如Paxos。但leader可以让raft更高效，每个follower都知道谁是leader，经过一轮消息的传递就可以处理一个事件。效率是最重要的原因，此外也有利于理解等其他原因。

**Raft编排leader的顺序**  
每当有一个新的leader选举出，就会有一个新的term任期。在一个任期内，最多只会有一个leader，或者没有。这个term的值让集群内的服务器知晓谁是当前的leader，而不会去跟随一个被取代了的leader。

**什么时候开始一轮新的选举？**  
每个节点都有一个`Election Timer`倒计时(每次收到消息，重置该时间)，倒计时内没有收到任何消息时，它就会发起一轮选举，当前的任期+1、向其他节点发出投票请求。此时，可能是它自己掉线，产生了一个没有意义的选举，也可能是旧的leader掉线了。

**如何确保每个term至多只有一个leader？**  
首先，必须获得集群中大多数节点的选票才能成为leader。其次，每个节点在当前的term中只能进行一次投票，如果它自己成为了候选人，它就投给自己；如果不是，它就投给第一个发送给它投票请求的节点。所以每个任期内，也至多只有一个节点能得到大多数其他节点的选票，即使在发生网络分裂、部分服务节点掉线的情况下。

**其他节点如何知晓一个新的leader是谁？**  
对于新leader自己，它获得了过半的选票，自然知道自己已经选举成功，但是其他leader是不知道这点的。只有leader能发出`AppendEntries`RPC请求，当follower收到一个更高term的该请求时，它们知道新的leader已经产生，不需要在进行下一轮的选举。

**选举失败**  
选举失败只有两种情况，其一是大部分节点的掉线导致不能拿到大多数的选票；其二是多个候选人同时投票，使得没有人能获得大多数选票。发生失败以后，其实还是会导致`Election Timer`超时，从而term+1，产生新的一轮选举。

**如何避免投票的分散？**  
每个节点会选择一个随机的选举超时，谁的时间先到谁就会先去发起投票请求，接收到请求的节点就会变成follower。这种通过随机延时的方式在网络协议中是很常见的。

**如何选择合适的选举超时时间？**  
这个时间不能太短，至少应该是几倍于心跳超时时间，这样如果网络中偶有丢包的话，可以避免不必要的选举。也需要足够长的时间让候选人在下一轮选举开始前计算出自己是否选举成功。也不能太长，因为在这个时间段，整个系统是没有leader的，也就是说是处于冻结状态无法响应客户端的。

**旧的leader不知道新的leader被选举出怎么办？**  
也许是由于网络波动，旧的leader和少部分节点的网络在处理一部分客户端请求，新的leader和大部分节点的网络在处理一部分客户端请求，旧的leader并不知道新的leader产生，这难道不会造成旧的leader错误的执行？  
旧的leader发出的`AppendEntries`RPC请求只会让少部分节点收到，那么它就不会去提交任何新的日志项目，也就不会去执行命令并返回给客户端。

日志
-------
只要leader存在，客户端只能和leader交互，而无法看到任何follower的状态或日志。那么当leader更替时，会不会出现一些异常？比如说存在两个不同的副本，丢失一些操作，重复一些操作等等。

**在发生crash后日志是如何同步的？**  
假设有三个节点，在某次s3(leader)还没有发送完`AppendEntries`时crash了，那么它们的日志情况可能是这样的:
```
S1: 3
S2: 3 3
S3: 3 3
```
之后，s2在term4被选举为leader并接收到一个客户端请求添加了一条日志后crash，s3在term5发生了同样的事情，会形成这种情况:
```
    10 11 12 13  <- log entry #  
S1:  3  
S2:  3  3  4  
S3:  3  3  5  
```
此时，s3在term6被选举为leader，它如何同步follower的日志？

s3下次发出的`AppendEntries`请求会包含`prevLogIndex=12, prevLogTerm=5, Entries=[log6]`，s2接收到请求以后发现prevLogTerm不匹配，返回false，同样s1也返回false。s3收到返回后，其nextIndex[s2]和nextIndex[s1]的值均减1变为12，下次发出的请求包含`prevLogIndex=11, prevLogTerm=3, Entries=[log5, log6]`，s2会接受这个请求以及相应的日志，删掉自己index12上的log，并返回true。同理，s1最终也会在下一次请求中得到同步。最终所有节点的日志得到同步，nextIndex[s2]和nextIndex[s1]的值变为14。